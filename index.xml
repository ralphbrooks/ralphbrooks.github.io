<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Best Practices</title>
    <link>https://ralphabrooks.com/</link>
    <description>Recent content on AI Best Practices</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Jun 2019 12:25:53 -0400</lastBuildDate>
    
	<atom:link href="https://ralphabrooks.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Learning Checklist</title>
      <link>https://ralphabrooks.com/deep_learning/keras/deep_learning_checklist/</link>
      <pubDate>Sun, 02 Jun 2019 12:25:53 -0400</pubDate>
      
      <guid>https://ralphabrooks.com/deep_learning/keras/deep_learning_checklist/</guid>
      <description> Define the problem Identify a way to reliably measure success against a goal Prepare a validation process that you will use to evaluate models Vectorize the data Develop a model that beats a trivial common sense baseline Refine model architecture Get your model to overfit  After overfitting use the following to refine model architecture
 Add regularization (dropout) to the model Downsize the model to use lower capacity  </description>
    </item>
    
    <item>
      <title>Freezing a layer in Keras</title>
      <link>https://ralphabrooks.com/deep_learning/keras/freezing_layers/</link>
      <pubDate>Tue, 28 May 2019 12:25:53 -0400</pubDate>
      
      <guid>https://ralphabrooks.com/deep_learning/keras/freezing_layers/</guid>
      <description>To freeze a layer in Keras, use:
model.layers[0].trainable = False Notes:
 Typically, the freezing of layers will be done so that weights which are learned in prior stages are not forgotten in later layers of the model. For example, if you have BERT as one part of a Keras TensorFlow model, that layer might need to be frozen so that large changes in gradient that occur during fine tuning do not distrupt the weights that have been learned in BERT.</description>
    </item>
    
    <item>
      <title>How to Set Environment Variables From A Linux Script</title>
      <link>https://ralphabrooks.com/linux/environment/setting_environment_variables_from_script/</link>
      <pubDate>Mon, 27 May 2019 12:25:53 -0400</pubDate>
      
      <guid>https://ralphabrooks.com/linux/environment/setting_environment_variables_from_script/</guid>
      <description>If you utilize the cloud, odds are that there has been a time where you spun up a machine and needed to set the environment variables from a linux script.
The process to do this is:
1) Create a file that contains environment variables that you want to export.
environ.sh
export APIKEY=&amp;#34;Sample API key&amp;#34; 2) Change the permissions of the environment file:
chmod +x environ.sh 3) Execute the script with a dot space prefix.</description>
    </item>
    
    <item>
      <title>Introduction and Links to NLP Demo</title>
      <link>https://ralphabrooks.com/articles/intro_and_links_to_demo/</link>
      <pubDate>Mon, 27 May 2019 12:25:53 -0400</pubDate>
      
      <guid>https://ralphabrooks.com/articles/intro_and_links_to_demo/</guid>
      <description>I am a data scientist and machine learning engineer in Frisco, TX where I am working on items at the interaction of deep learning, natural language processing, and Bayesian statistics.
Currently, I am working on ways to detect emotion in language. Some of my work is at:
 Toxic Classififer Detection of sentiment vs neutral  </description>
    </item>
    
    <item>
      <title>Using a confusion matrix within Keras</title>
      <link>https://ralphabrooks.com/deep_learning/keras/using_confusion_matrix_with_keras/</link>
      <pubDate>Mon, 27 May 2019 12:25:53 -0400</pubDate>
      
      <guid>https://ralphabrooks.com/deep_learning/keras/using_confusion_matrix_with_keras/</guid>
      <description>Assuming that y_label are the actual gold standard labels that you want to evaluate, you can use sklearn&amp;rsquo;s confusion matrix function in order to evaluate a keras model.
Code would look like the following
import numpy as np from sklearn.metrics import confusion_matrix model = &amp;lt;use your favorite keras model here&amp;gt; y_pred = model.predict(test_data, test_labels) y_pred_max = np.apply_along_axis(lambda x : np.argmax(x) +1, axis =1, arr=y_pred) cm = confusion_matrix(test_label, y_pred_max)</description>
    </item>
    
    <item>
      <title>About Ralph Brooks</title>
      <link>https://ralphabrooks.com/about/ralph_brooks/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ralphabrooks.com/about/ralph_brooks/</guid>
      <description>I am a data scientist and machine learning engineer in Frisco, TX.
 Twitter: @ralphbrooks LinkedIn: https://www.linkedin.com/in/ralphbrooks/  Currently, I am working on data science as well as building out the infrastructure needed to do distributed deep learning.</description>
    </item>
    
  </channel>
</rss>